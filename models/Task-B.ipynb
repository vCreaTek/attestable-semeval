{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemEval_Task_B_using_sentence_transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OAjz0zP65z-3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BvB2MYPBbIw"
      },
      "source": [
        "# Evidence Finding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAjz0zP65z-3"
      },
      "source": [
        "# Install the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqat7Xsxdcu7"
      },
      "source": [
        "!pip install transformers sentencepiece\n",
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gi4oVUm5aVT"
      },
      "source": [
        "# Mount GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4lcuIizRD_j"
      },
      "source": [
        "# Mount Google drive to access the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBaahyrjYFiv"
      },
      "source": [
        "# Data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4VQg35S56Xk"
      },
      "source": [
        "Read the data from the csv file. The original file has 10.24M samples, but here we are using only some of them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mv7R0xR6Gz2"
      },
      "source": [
        "import torch\n",
        "# from torchtext.data import Field, TabularDataset, BucketIterator, Dataset, Example\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import random\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G6ZCqk9Yo2b"
      },
      "source": [
        "PATH_DATA = \"/content/drive/MyDrive/SemTabFact/data_task_b_undersampled_60.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5uo_ri8YmyM"
      },
      "source": [
        "df = pd.read_csv(\n",
        "    PATH_DATA, \n",
        "    index_col = 0\n",
        ")\n",
        "df.index.name = \"id\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d66v-I1Z0Us"
      },
      "source": [
        "df = df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwMT_QY9Y-NR"
      },
      "source": [
        "df_sample = df.sample(n = 1000000, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy1Ch77kYlxJ"
      },
      "source": [
        "df_sample[\"relevancy\"] = df_sample[\"relevancy\"].astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgSXHKxi6juP"
      },
      "source": [
        "display(df_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dPSTQLX6qt5"
      },
      "source": [
        "df_sample[\"relevancy\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVG_4u42sMlt"
      },
      "source": [
        "# Split the data in training and validation\n",
        "df_train, df_val = tts(df_sample, shuffle = True, train_size = 0.8, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j70cuFSv5_2P"
      },
      "source": [
        "display(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8IYdBIqOOSr"
      },
      "source": [
        "# Check if GPU is available or not\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh2jSUdNk8or"
      },
      "source": [
        "# Class Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yDgiJw47FmP"
      },
      "source": [
        "df_train[\"relevancy\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnRBQxx87Ix-"
      },
      "source": [
        "df_val[\"relevancy\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRM8JQwVyuEZ"
      },
      "source": [
        "# Sentence Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7fLQ_MOBqCC"
      },
      "source": [
        "from sentence_transformers.losses import CosineSimilarityLoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoUjzxuCXiNt"
      },
      "source": [
        "def weightedMSELoss(input, target):\n",
        "    mse = (input - target)**2 # [64]\n",
        "\n",
        "    weights = torch.ones(target.size())\n",
        "    weights[target == 1] = 2 # More weight to minority class\n",
        "    weights = weights.to(device)\n",
        "\n",
        "    assert mse.size() == weights.size()\n",
        "    loss = weights*mse\n",
        "    return loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUUTAsXQa6AP"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, InputExample, evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlfzqiGsyxBR"
      },
      "source": [
        "model = SentenceTransformer(\"/content/drive/MyDrive/SemTabFact/stsb_weighted_2_csl_1M_epoch2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr56Lfjjax7O"
      },
      "source": [
        "list_sentence1 = df_train[\"cell_text\"].values.tolist()\n",
        "list_sentence2 = df_train[\"statement_text\"].values.tolist()\n",
        "list_labels = df_train[\"relevancy\"].values.tolist()\n",
        "\n",
        "train_examples = []\n",
        "for i in range(len(list_labels)) :\n",
        "    texts = []\n",
        "    texts.append(str(list_sentence1[i]))\n",
        "    texts.append(str(list_sentence2[i]))\n",
        "    train_examples.append(InputExample(texts=texts, label=list_labels[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB9dH8ndbPWW"
      },
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO6MRnR2bVos"
      },
      "source": [
        "N_BATCH = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6x05yIhbAqj"
      },
      "source": [
        "# Define the train dataset, the dataloader and the train loss\n",
        "train_dataloader = DataLoader(\n",
        "    train_examples, \n",
        "    shuffle = False, \n",
        "    batch_size = N_BATCH\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnmZq7ascKpf"
      },
      "source": [
        "train_loss = CosineSimilarityLoss(model, loss_fct = weightedMSELoss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_TEdZT53S4B"
      },
      "source": [
        "evaluator = evaluation.EmbeddingSimilarityEvaluator(\n",
        "    df_val[\"cell_text\"].values.astype(str), \n",
        "    df_val[\"statement_text\"].values.astype(str), \n",
        "    df_val[\"relevancy\"].values\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoRc0nyfvoO6"
      },
      "source": [
        "# Tune the model\n",
        "model.fit(\n",
        "    train_objectives = [(train_dataloader, train_loss)],\n",
        "    epochs = 1,\n",
        "    warmup_steps = 100,\n",
        "    # evaluator = evaluator,\n",
        "    # evaluation_steps = 500,\n",
        "    output_path = \"/content/drive/MyDrive/SemTabFact/stsb_weighted_2_csl_1M_epoch3.h5\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpobgkuXzuw0"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/SemTabFact/stsb_weighted_2_csl_1M_epoch3.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7yiQT4tpzFg"
      },
      "source": [
        "model.evaluate(evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O3xi-qc2Ona"
      },
      "source": [
        "val_list_sentence1 = df_val[\"cell_text\"].values.astype(str).tolist()\n",
        "val_list_sentence2 = df_val[\"statement_text\"].values.astype(str).tolist()\n",
        "val_list_labels = df_val[\"relevancy\"].values.astype(int).tolist()\n",
        "\n",
        "val_examples = []\n",
        "for i in range(len(val_list_labels)) :\n",
        "    texts = []\n",
        "    texts.append(str(val_list_sentence1[i]))\n",
        "    texts.append(str(val_list_sentence2[i]))\n",
        "    val_examples.append(InputExample(texts=texts, label=val_list_labels[i]))\n",
        "\n",
        "# Define the train dataset, the dataloader and the train loss\n",
        "val_dataloader = DataLoader(val_examples, shuffle=True, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95RUyBV41gq-"
      },
      "source": [
        "acc_evaluator = evaluation.BinaryClassificationEvaluator(val_list_sentence1, val_list_sentence2, val_list_labels)\n",
        "model.evaluate(acc_evaluator)\n",
        "# Gives Average Precision with Cosine-Similarity"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}